=== GPT-only pipeline for Math-90 ===
  📊 GPT[hypothesis H1] tokens: 75 prompt + 51 completion = 126 total
  📊 GPT[hypothesis H2] tokens: 75 prompt + 38 completion = 113 total
  📊 GPT[hypothesis H3] tokens: 75 prompt + 38 completion = 113 total
  📊 GPT[hypothesis H4] tokens: 75 prompt + 43 completion = 118 total
  📊 GPT[hypothesis H5] tokens: 75 prompt + 43 completion = 118 total
  📊 GPT[hypothesis_confidence H1] tokens: 112 prompt + 3 completion = 115 total
  📊 GPT[hypothesis_confidence H2] tokens: 99 prompt + 3 completion = 102 total
  📊 GPT[hypothesis_confidence H3] tokens: 99 prompt + 3 completion = 102 total
  📊 GPT[hypothesis_confidence H4] tokens: 104 prompt + 3 completion = 107 total
  📊 GPT[hypothesis_confidence H5] tokens: 104 prompt + 3 completion = 107 total
Hypotheses:
  H1 (confidence 0.900): H1: The failure in "org.apache.commons.math.stat.FrequencyTest::testAddNonComparable" may be caused by the method attempting to add elements that do not implement the Comparable interface, leading to a ClassCastException during sorting or comparison operations.
  H2 (confidence 0.700): Hypothesis H2: The failure may be caused by the `Frequency` class not properly handling or rejecting non-comparable objects, leading to a runtime exception when attempting to add such objects.
  H3 (confidence 0.700): Hypothesis H3: The failure may be caused by the `Frequency` class not properly handling or rejecting non-comparable objects, leading to an exception or incorrect behavior during the test execution.
  H4 (confidence 0.700): Hypothesis H4: The failure may be caused by the `Frequency` class not properly handling or rejecting non-comparable objects, leading to a `ClassCastException` or similar runtime error during the test execution.
  H5 (confidence 0.700): Hypothesis H5: The failure may be caused by the `Frequency` class not properly handling or rejecting non-comparable objects, leading to a `ClassCastException` or similar runtime error during the test execution.
    ▶️ GPT[class pre-ranking] running 1 prompts
  📊 GPT[class_pre_rank org.apache.commons.math.stat.Frequency] tokens: 535 prompt + 78 completion = 613 total
    ✅ GPT[class pre-ranking] completed; taking top 5
Candidate classes:
  org.apache.commons.math.stat.Frequency: n/a ```json
{"score": 0.9, "reason": "The failure occurs in the `addValue(Object)` method of the `Frequency` class, which is responsible for handling the addition of values. The stack trace and test indicate that the method throws an `IllegalArgumentException` instead of the expected `ClassCastException` when non-comparable objects are added."}
```
Collected 2 methods across candidate classes
    ▶️ GPT[method pre-ranking] running 2 prompts
  📊 GPT[method_pre_rank org.apache.commons.math.stat.Frequency.Frequency()] tokens: 541 prompt + 64 completion = 605 total
  📊 GPT[method_pre_rank org.apache.commons.math.stat.Frequency.addValue(Object)] tokens: 710 prompt + 81 completion = 791 total
    ✅ GPT[method pre-ranking] completed
Selected 2 candidate methods
  📊 GPT[class_score org.apache.commons.math.stat.Frequency H1] tokens: 337 prompt + 3 completion = 340 total
  📊 GPT[class_explanation org.apache.commons.math.stat.Frequency H1] tokens: 314 prompt + 128 completion = 442 total
  📊 GPT[class_score org.apache.commons.math.stat.Frequency H2] tokens: 324 prompt + 3 completion = 327 total
  📊 GPT[class_explanation org.apache.commons.math.stat.Frequency H2] tokens: 301 prompt + 96 completion = 397 total
  📊 GPT[class_score org.apache.commons.math.stat.Frequency H3] tokens: 324 prompt + 3 completion = 327 total
  📊 GPT[class_explanation org.apache.commons.math.stat.Frequency H3] tokens: 301 prompt + 112 completion = 413 total
  📊 GPT[class_score org.apache.commons.math.stat.Frequency H4] tokens: 329 prompt + 3 completion = 332 total
  📊 GPT[class_explanation org.apache.commons.math.stat.Frequency H4] tokens: 306 prompt + 109 completion = 415 total
  📊 GPT[class_score org.apache.commons.math.stat.Frequency H5] tokens: 329 prompt + 3 completion = 332 total
  📊 GPT[class_explanation org.apache.commons.math.stat.Frequency H5] tokens: 306 prompt + 116 completion = 422 total
  📊 GPT[method_score org.apache.commons.math.stat.Frequency.addValue(Object) H1] tokens: 580 prompt + 3 completion = 583 total
  📊 GPT[method_explanation org.apache.commons.math.stat.Frequency.addValue(Object) H1] tokens: 437 prompt + 144 completion = 581 total
  📊 GPT[method_score org.apache.commons.math.stat.Frequency.Frequency() H1] tokens: 362 prompt + 3 completion = 365 total
  📊 GPT[method_explanation org.apache.commons.math.stat.Frequency.Frequency() H1] tokens: 337 prompt + 108 completion = 445 total
  📊 GPT[method_score org.apache.commons.math.stat.Frequency.addValue(Object) H2] tokens: 567 prompt + 3 completion = 570 total
  📊 GPT[method_explanation org.apache.commons.math.stat.Frequency.addValue(Object) H2] tokens: 424 prompt + 129 completion = 553 total
  📊 GPT[method_score org.apache.commons.math.stat.Frequency.Frequency() H2] tokens: 349 prompt + 3 completion = 352 total
  📊 GPT[method_explanation org.apache.commons.math.stat.Frequency.Frequency() H2] tokens: 324 prompt + 110 completion = 434 total
  📊 GPT[method_score org.apache.commons.math.stat.Frequency.addValue(Object) H3] tokens: 567 prompt + 3 completion = 570 total
  📊 GPT[method_explanation org.apache.commons.math.stat.Frequency.addValue(Object) H3] tokens: 424 prompt + 107 completion = 531 total
  📊 GPT[method_score org.apache.commons.math.stat.Frequency.Frequency() H3] tokens: 349 prompt + 3 completion = 352 total
  📊 GPT[method_explanation org.apache.commons.math.stat.Frequency.Frequency() H3] tokens: 324 prompt + 105 completion = 429 total
  📊 GPT[method_score org.apache.commons.math.stat.Frequency.addValue(Object) H4] tokens: 572 prompt + 3 completion = 575 total
  📊 GPT[method_explanation org.apache.commons.math.stat.Frequency.addValue(Object) H4] tokens: 429 prompt + 115 completion = 544 total
  📊 GPT[method_score org.apache.commons.math.stat.Frequency.Frequency() H4] tokens: 354 prompt + 3 completion = 357 total
  📊 GPT[method_explanation org.apache.commons.math.stat.Frequency.Frequency() H4] tokens: 329 prompt + 106 completion = 435 total
  📊 GPT[method_score org.apache.commons.math.stat.Frequency.addValue(Object) H5] tokens: 572 prompt + 3 completion = 575 total
  📊 GPT[method_explanation org.apache.commons.math.stat.Frequency.addValue(Object) H5] tokens: 429 prompt + 127 completion = 556 total
  📊 GPT[method_score org.apache.commons.math.stat.Frequency.Frequency() H5] tokens: 354 prompt + 3 completion = 357 total
  📊 GPT[method_explanation org.apache.commons.math.stat.Frequency.Frequency() H5] tokens: 329 prompt + 104 completion = 433 total

Top suspicious methods:
  1. org.apache.commons.math.stat.Frequency.addValue(Object): 0.900 — best hypothesis H1: H1: The failure in "org.apache.commons.math.stat.FrequencyTest::testAddNonComparable" may be caused by the method attempting to add elements that do not implement the Comparable interface, leading to a ClassCastException during sorting or comparison operations. (confidence 0.900); supporting class org.apache.commons.math.stat.Frequency (HH1)
      explanation: The method `org.apache.commons.math.stat.Frequency.addValue(Object)` supports hypothesis H1 by requiring that any object added must be comparable to previously added objects, as indicated by the method's summary and the `IllegalArgumentE...
  2. org.apache.commons.math.stat.Frequency.Frequency(): 0.200 — best hypothesis H1: H1: The failure in "org.apache.commons.math.stat.FrequencyTest::testAddNonComparable" may be caused by the method attempting to add elements that do not implement the Comparable interface, leading to a ClassCastException during sorting or comparison operations. (confidence 0.900); supporting class org.apache.commons.math.stat.Frequency (HH1)
      explanation: The method `org.apache.commons.math.stat.Frequency.Frequency()` initializes the internal frequency table using a `TreeMap`, which inherently requires elements to be `Comparable` for sorting purposes. This supports hypothesis H1, as attem...

📊 Token Usage Summary:
  Total API calls: 43
  Total tokens: 16,474
  Prompt tokens: 14,262
  Completion tokens: 2,212
Results written to defects4j_batch_results/Math-90_parallel_case/Math-90_parallel_answer.csv
Token usage written to defects4j_batch_results/Math-90_parallel_case/Math-90_token_usage.csv
Summary written to defects4j_batch_results/Math-90_parallel_case/Math-90_parallel_summary.md
