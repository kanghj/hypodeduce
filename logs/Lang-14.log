=== GPT-only pipeline for Lang-14 ===
  📊 GPT[hypothesis H1] tokens: 76 prompt + 51 completion = 127 total
  📊 GPT[hypothesis H2] tokens: 76 prompt + 51 completion = 127 total
  📊 GPT[hypothesis H3] tokens: 76 prompt + 37 completion = 113 total
  📊 GPT[hypothesis H4] tokens: 76 prompt + 29 completion = 105 total
  📊 GPT[hypothesis H5] tokens: 76 prompt + 50 completion = 126 total
  📊 GPT[hypothesis_confidence H1] tokens: 112 prompt + 3 completion = 115 total
  📊 GPT[hypothesis_confidence H2] tokens: 112 prompt + 3 completion = 115 total
  📊 GPT[hypothesis_confidence H3] tokens: 98 prompt + 3 completion = 101 total
  📊 GPT[hypothesis_confidence H4] tokens: 90 prompt + 3 completion = 93 total
  📊 GPT[hypothesis_confidence H5] tokens: 111 prompt + 3 completion = 114 total
Hypotheses:
  H1 (confidence 0.700): Hypothesis H1: The failure in "org.apache.commons.lang3.StringUtilsEqualsIndexOfTest::testEquals" could be due to a recent change in the StringUtils.equals method that incorrectly handles null values, leading to unexpected behavior during the test.
  H2 (confidence 0.700): Hypothesis H2: The failure in "org.apache.commons.lang3.StringUtilsEqualsIndexOfTest::testEquals" could be due to a recent change in the StringUtils.equals method that introduced a bug when handling null inputs or specific character encodings.
  H3 (confidence 0.700): Hypothesis H3: The failure might be caused by a recent change in the `StringUtils.equals` method that introduced a bug when comparing strings with special characters or different encodings.
  H4 (confidence 0.700): Hypothesis H4: The failure might be caused by a recent change in the StringUtils.equals method that introduced a bug affecting string comparison logic.
  H5 (confidence 0.800): Hypothesis H5: The failure in "org.apache.commons.lang3.StringUtilsEqualsIndexOfTest::testEquals" could be due to a recent change in the StringUtils.equals method that inadvertently altered its behavior when handling null or empty string inputs.
    ▶️ GPT[class pre-ranking] running 1 prompts
  📊 GPT[class_pre_rank org.apache.commons.lang3.StringUtils] tokens: 601 prompt + 47 completion = 648 total
    ✅ GPT[class pre-ranking] completed; taking top 5
Candidate classes:
  org.apache.commons.lang3.StringUtils: n/a ```json
{"score": 0.9, "reason": "The failure occurs in the StringUtils.equals method when comparing a CharSequence with a StringBuilder, indicating a likely issue in handling different CharSequence implementations."}
```
Collected 1 methods across candidate classes
    ▶️ GPT[method pre-ranking] running 1 prompts
  📊 GPT[method_pre_rank org.apache.commons.lang3.StringUtils.equals(CharSequence,CharSequence)] tokens: 716 prompt + 73 completion = 789 total
    ✅ GPT[method pre-ranking] completed
Selected 1 candidate methods
  📊 GPT[class_score org.apache.commons.lang3.StringUtils H1] tokens: 436 prompt + 3 completion = 439 total
  📊 GPT[class_explanation org.apache.commons.lang3.StringUtils H1] tokens: 414 prompt + 118 completion = 532 total
  📊 GPT[class_score org.apache.commons.lang3.StringUtils H2] tokens: 436 prompt + 3 completion = 439 total
  📊 GPT[class_explanation org.apache.commons.lang3.StringUtils H2] tokens: 414 prompt + 146 completion = 560 total
  📊 GPT[class_score org.apache.commons.lang3.StringUtils H3] tokens: 422 prompt + 3 completion = 425 total
  📊 GPT[class_explanation org.apache.commons.lang3.StringUtils H3] tokens: 400 prompt + 113 completion = 513 total
  📊 GPT[class_score org.apache.commons.lang3.StringUtils H4] tokens: 414 prompt + 3 completion = 417 total
  📊 GPT[class_explanation org.apache.commons.lang3.StringUtils H4] tokens: 392 prompt + 107 completion = 499 total
  📊 GPT[class_score org.apache.commons.lang3.StringUtils H5] tokens: 435 prompt + 3 completion = 438 total
  📊 GPT[class_explanation org.apache.commons.lang3.StringUtils H5] tokens: 413 prompt + 151 completion = 564 total
  📊 GPT[method_score org.apache.commons.lang3.StringUtils.equals(CharSequence,CharSequence) H1] tokens: 575 prompt + 3 completion = 578 total
  📊 GPT[method_explanation org.apache.commons.lang3.StringUtils.equals(CharSequence,CharSequence) H1] tokens: 538 prompt + 131 completion = 669 total
  📊 GPT[method_score org.apache.commons.lang3.StringUtils.equals(CharSequence,CharSequence) H2] tokens: 575 prompt + 3 completion = 578 total
  📊 GPT[method_explanation org.apache.commons.lang3.StringUtils.equals(CharSequence,CharSequence) H2] tokens: 538 prompt + 139 completion = 677 total
  📊 GPT[method_score org.apache.commons.lang3.StringUtils.equals(CharSequence,CharSequence) H3] tokens: 561 prompt + 3 completion = 564 total
  📊 GPT[method_explanation org.apache.commons.lang3.StringUtils.equals(CharSequence,CharSequence) H3] tokens: 524 prompt + 119 completion = 643 total
  📊 GPT[method_score org.apache.commons.lang3.StringUtils.equals(CharSequence,CharSequence) H4] tokens: 553 prompt + 3 completion = 556 total
  📊 GPT[method_explanation org.apache.commons.lang3.StringUtils.equals(CharSequence,CharSequence) H4] tokens: 516 prompt + 146 completion = 662 total
  📊 GPT[method_score org.apache.commons.lang3.StringUtils.equals(CharSequence,CharSequence) H5] tokens: 574 prompt + 3 completion = 577 total
  📊 GPT[method_explanation org.apache.commons.lang3.StringUtils.equals(CharSequence,CharSequence) H5] tokens: 537 prompt + 139 completion = 676 total

Top suspicious methods:
  1. org.apache.commons.lang3.StringUtils.equals(CharSequence,CharSequence): 0.800 — best hypothesis H1: Hypothesis H1: The failure in "org.apache.commons.lang3.StringUtilsEqualsIndexOfTest::testEquals" could be due to a recent change in the StringUtils.equals method that incorrectly handles null values, leading to unexpected behavior during the test. (confidence 0.700); supporting class org.apache.commons.lang3.StringUtils (HH3)
      explanation: The method `org.apache.commons.lang3.StringUtils.equals(CharSequence, CharSequence)` handles `null` values correctly by returning `true` if both inputs are `null` and `false` if only one is `null`, which aligns with the expected behavior...

📊 Token Usage Summary:
  Total API calls: 32
  Total tokens: 13,579
  Prompt tokens: 11,887
  Completion tokens: 1,692
Results written to defects4j_batch_results/Lang-14_parallel_case/Lang-14_parallel_answer.csv
Token usage written to defects4j_batch_results/Lang-14_parallel_case/Lang-14_token_usage.csv
Summary written to defects4j_batch_results/Lang-14_parallel_case/Lang-14_parallel_summary.md
