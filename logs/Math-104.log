=== GPT-only pipeline for Math-104 ===
  📊 GPT[hypothesis H1] tokens: 77 prompt + 59 completion = 136 total
  📊 GPT[hypothesis H2] tokens: 77 prompt + 40 completion = 117 total
  📊 GPT[hypothesis H3] tokens: 77 prompt + 40 completion = 117 total
  📊 GPT[hypothesis H4] tokens: 77 prompt + 40 completion = 117 total
  📊 GPT[hypothesis H5] tokens: 77 prompt + 40 completion = 117 total
  📊 GPT[hypothesis_confidence H1] tokens: 120 prompt + 3 completion = 123 total
  📊 GPT[hypothesis_confidence H2] tokens: 101 prompt + 3 completion = 104 total
  📊 GPT[hypothesis_confidence H3] tokens: 101 prompt + 3 completion = 104 total
  📊 GPT[hypothesis_confidence H4] tokens: 101 prompt + 3 completion = 104 total
  📊 GPT[hypothesis_confidence H5] tokens: 101 prompt + 3 completion = 104 total
Hypotheses:
  H1 (confidence 0.700): Hypothesis H1: The failure in "org.apache.commons.math.special.GammaTest::testRegularizedGammaPositivePositive" could be due to incorrect handling of edge cases where the input values are very close to zero, leading to numerical instability or precision errors in the regularized gamma function calculation.
  H2 (confidence 0.700): Hypothesis H2: The failure might be caused by incorrect handling of edge cases where the input values for the regularized gamma function are extremely close to zero, leading to numerical instability or precision errors.
  H3 (confidence 0.700): Hypothesis H3: The failure might be caused by incorrect handling of edge cases where the input values for the regularized gamma function are extremely close to zero, leading to numerical instability or precision errors.
  H4 (confidence 0.700): Hypothesis H4: The failure might be caused by incorrect handling of edge cases where the input values for the regularized gamma function are extremely close to zero, leading to numerical instability or precision errors.
  H5 (confidence 0.700): Hypothesis H5: The failure might be caused by incorrect handling of edge cases where the input values for the regularized gamma function are extremely close to zero, leading to numerical instability or precision errors.
    ▶️ GPT[class pre-ranking] running 2 prompts
  📊 GPT[class_pre_rank org.apache.commons.math.util.ContinuedFraction] tokens: 539 prompt + 61 completion = 600 total
  📊 GPT[class_pre_rank org.apache.commons.math.special.Gamma] tokens: 574 prompt + 65 completion = 639 total
    ✅ GPT[class pre-ranking] completed; taking top 5
Candidate classes:
  org.apache.commons.math.util.ContinuedFraction: n/a ```json
{"score": 0.8, "reason": "The failure is due to a precision error in the result of the regularized gamma function, which relies on the evaluate method of ContinuedFraction. This suggests the issue likely resides in the ContinuedFraction class's handling of precision."}
```
  org.apache.commons.math.special.Gamma: n/a ```json
{"score": 0.9, "reason": "The failure occurs in the test for the regularized gamma function, specifically in the method 'regularizedGammaP', which is part of the 'Gamma' class. The discrepancy between expected and actual values suggests a precision issue in this method."}
```
Collected 6 methods across candidate classes
    ▶️ GPT[method pre-ranking] running 6 prompts
  📊 GPT[method_pre_rank org.apache.commons.math.special.Gamma.logGamma(double)] tokens: 514 prompt + 70 completion = 584 total
  📊 GPT[method_pre_rank org.apache.commons.math.special.Gamma.regularizedGammaP(double,double)] tokens: 576 prompt + 71 completion = 647 total
  📊 GPT[method_pre_rank org.apache.commons.math.special.Gamma.regularizedGammaP(double,double,double,int)] tokens: 602 prompt + 80 completion = 682 total
  📊 GPT[method_pre_rank org.apache.commons.math.special.Gamma.regularizedGammaQ(double,double)] tokens: 528 prompt + 75 completion = 603 total
  📊 GPT[method_pre_rank org.apache.commons.math.special.Gamma.regularizedGammaQ(double,double,double,int)] tokens: 555 prompt + 66 completion = 621 total
  📊 GPT[method_pre_rank org.apache.commons.math.util.ContinuedFraction.evaluate(double,double,int)] tokens: 667 prompt + 64 completion = 731 total
    ✅ GPT[method pre-ranking] completed
Selected 6 candidate methods
  📊 GPT[class_score org.apache.commons.math.util.ContinuedFraction H1] tokens: 330 prompt + 3 completion = 333 total
  📊 GPT[class_explanation org.apache.commons.math.util.ContinuedFraction H1] tokens: 308 prompt + 114 completion = 422 total
  📊 GPT[class_score org.apache.commons.math.special.Gamma H1] tokens: 359 prompt + 3 completion = 362 total
  📊 GPT[class_explanation org.apache.commons.math.special.Gamma H1] tokens: 337 prompt + 109 completion = 446 total
  📊 GPT[class_score org.apache.commons.math.util.ContinuedFraction H2] tokens: 311 prompt + 3 completion = 314 total
  📊 GPT[class_explanation org.apache.commons.math.util.ContinuedFraction H2] tokens: 289 prompt + 118 completion = 407 total
  📊 GPT[class_score org.apache.commons.math.special.Gamma H2] tokens: 340 prompt + 3 completion = 343 total
  📊 GPT[class_explanation org.apache.commons.math.special.Gamma H2] tokens: 318 prompt + 123 completion = 441 total
  📊 GPT[class_score org.apache.commons.math.util.ContinuedFraction H3] tokens: 311 prompt + 3 completion = 314 total
  📊 GPT[class_explanation org.apache.commons.math.util.ContinuedFraction H3] tokens: 289 prompt + 116 completion = 405 total
  📊 GPT[class_score org.apache.commons.math.special.Gamma H3] tokens: 340 prompt + 3 completion = 343 total
  📊 GPT[class_explanation org.apache.commons.math.special.Gamma H3] tokens: 318 prompt + 116 completion = 434 total
  📊 GPT[class_score org.apache.commons.math.util.ContinuedFraction H4] tokens: 311 prompt + 3 completion = 314 total
  📊 GPT[class_explanation org.apache.commons.math.util.ContinuedFraction H4] tokens: 289 prompt + 115 completion = 404 total
  📊 GPT[class_score org.apache.commons.math.special.Gamma H4] tokens: 340 prompt + 3 completion = 343 total
  📊 GPT[class_explanation org.apache.commons.math.special.Gamma H4] tokens: 318 prompt + 107 completion = 425 total
  📊 GPT[class_score org.apache.commons.math.util.ContinuedFraction H5] tokens: 311 prompt + 3 completion = 314 total
  📊 GPT[class_explanation org.apache.commons.math.util.ContinuedFraction H5] tokens: 289 prompt + 111 completion = 400 total
  📊 GPT[class_score org.apache.commons.math.special.Gamma H5] tokens: 340 prompt + 3 completion = 343 total
  📊 GPT[class_explanation org.apache.commons.math.special.Gamma H5] tokens: 318 prompt + 113 completion = 431 total
  📊 GPT[method_score org.apache.commons.math.util.ContinuedFraction.evaluate(double,double,int) H1] tokens: 551 prompt + 3 completion = 554 total
  📊 GPT[method_explanation org.apache.commons.math.util.ContinuedFraction.evaluate(double,double,int) H1] tokens: 413 prompt + 132 completion = 545 total
  📊 GPT[method_score org.apache.commons.math.special.Gamma.regularizedGammaP(double,double,double,int) H1] tokens: 507 prompt + 3 completion = 510 total
  📊 GPT[method_explanation org.apache.commons.math.special.Gamma.regularizedGammaP(double,double,double,int) H1] tokens: 368 prompt + 146 completion = 514 total
  📊 GPT[method_score org.apache.commons.math.special.Gamma.regularizedGammaQ(double,double) H1] tokens: 372 prompt + 3 completion = 375 total
  📊 GPT[method_explanation org.apache.commons.math.special.Gamma.regularizedGammaQ(double,double) H1] tokens: 347 prompt + 118 completion = 465 total
  📊 GPT[method_score org.apache.commons.math.special.Gamma.regularizedGammaQ(double,double,double,int) H1] tokens: 398 prompt + 3 completion = 401 total
  📊 GPT[method_explanation org.apache.commons.math.special.Gamma.regularizedGammaQ(double,double,double,int) H1] tokens: 373 prompt + 151 completion = 524 total
  📊 GPT[method_score org.apache.commons.math.special.Gamma.logGamma(double) H1] tokens: 357 prompt + 3 completion = 360 total
  📊 GPT[method_explanation org.apache.commons.math.special.Gamma.logGamma(double) H1] tokens: 332 prompt + 111 completion = 443 total
  📊 GPT[method_score org.apache.commons.math.special.Gamma.regularizedGammaP(double,double) H1] tokens: 423 prompt + 3 completion = 426 total
  📊 GPT[method_explanation org.apache.commons.math.special.Gamma.regularizedGammaP(double,double) H1] tokens: 401 prompt + 151 completion = 552 total
  📊 GPT[method_score org.apache.commons.math.util.ContinuedFraction.evaluate(double,double,int) H2] tokens: 532 prompt + 3 completion = 535 total
  📊 GPT[method_explanation org.apache.commons.math.util.ContinuedFraction.evaluate(double,double,int) H2] tokens: 394 prompt + 136 completion = 530 total
  📊 GPT[method_score org.apache.commons.math.special.Gamma.regularizedGammaP(double,double,double,int) H2] tokens: 488 prompt + 3 completion = 491 total
  📊 GPT[method_explanation org.apache.commons.math.special.Gamma.regularizedGammaP(double,double,double,int) H2] tokens: 349 prompt + 189 completion = 538 total
  📊 GPT[method_score org.apache.commons.math.special.Gamma.regularizedGammaQ(double,double) H2] tokens: 353 prompt + 3 completion = 356 total
  📊 GPT[method_explanation org.apache.commons.math.special.Gamma.regularizedGammaQ(double,double) H2] tokens: 328 prompt + 135 completion = 463 total
  📊 GPT[method_score org.apache.commons.math.special.Gamma.regularizedGammaQ(double,double,double,int) H2] tokens: 379 prompt + 3 completion = 382 total
  📊 GPT[method_explanation org.apache.commons.math.special.Gamma.regularizedGammaQ(double,double,double,int) H2] tokens: 354 prompt + 135 completion = 489 total
  📊 GPT[method_score org.apache.commons.math.special.Gamma.logGamma(double) H2] tokens: 338 prompt + 3 completion = 341 total
  📊 GPT[method_explanation org.apache.commons.math.special.Gamma.logGamma(double) H2] tokens: 313 prompt + 126 completion = 439 total
  📊 GPT[method_score org.apache.commons.math.special.Gamma.regularizedGammaP(double,double) H2] tokens: 404 prompt + 3 completion = 407 total
  📊 GPT[method_explanation org.apache.commons.math.special.Gamma.regularizedGammaP(double,double) H2] tokens: 382 prompt + 139 completion = 521 total
  📊 GPT[method_score org.apache.commons.math.util.ContinuedFraction.evaluate(double,double,int) H3] tokens: 532 prompt + 3 completion = 535 total
  📊 GPT[method_explanation org.apache.commons.math.util.ContinuedFraction.evaluate(double,double,int) H3] tokens: 394 prompt + 120 completion = 514 total
  📊 GPT[method_score org.apache.commons.math.special.Gamma.regularizedGammaP(double,double,double,int) H3] tokens: 488 prompt + 3 completion = 491 total
  📊 GPT[method_explanation org.apache.commons.math.special.Gamma.regularizedGammaP(double,double,double,int) H3] tokens: 349 prompt + 155 completion = 504 total
  📊 GPT[method_score org.apache.commons.math.special.Gamma.regularizedGammaQ(double,double) H3] tokens: 353 prompt + 3 completion = 356 total
  📊 GPT[method_explanation org.apache.commons.math.special.Gamma.regularizedGammaQ(double,double) H3] tokens: 328 prompt + 120 completion = 448 total
  📊 GPT[method_score org.apache.commons.math.special.Gamma.regularizedGammaQ(double,double,double,int) H3] tokens: 379 prompt + 3 completion = 382 total
  📊 GPT[method_explanation org.apache.commons.math.special.Gamma.regularizedGammaQ(double,double,double,int) H3] tokens: 354 prompt + 155 completion = 509 total
  📊 GPT[method_score org.apache.commons.math.special.Gamma.logGamma(double) H3] tokens: 338 prompt + 3 completion = 341 total
  📊 GPT[method_explanation org.apache.commons.math.special.Gamma.logGamma(double) H3] tokens: 313 prompt + 113 completion = 426 total
  📊 GPT[method_score org.apache.commons.math.special.Gamma.regularizedGammaP(double,double) H3] tokens: 404 prompt + 3 completion = 407 total
  📊 GPT[method_explanation org.apache.commons.math.special.Gamma.regularizedGammaP(double,double) H3] tokens: 382 prompt + 116 completion = 498 total
  📊 GPT[method_score org.apache.commons.math.util.ContinuedFraction.evaluate(double,double,int) H4] tokens: 532 prompt + 3 completion = 535 total
  📊 GPT[method_explanation org.apache.commons.math.util.ContinuedFraction.evaluate(double,double,int) H4] tokens: 394 prompt + 138 completion = 532 total
  📊 GPT[method_score org.apache.commons.math.special.Gamma.regularizedGammaP(double,double,double,int) H4] tokens: 488 prompt + 3 completion = 491 total
  📊 GPT[method_explanation org.apache.commons.math.special.Gamma.regularizedGammaP(double,double,double,int) H4] tokens: 349 prompt + 142 completion = 491 total
  📊 GPT[method_score org.apache.commons.math.special.Gamma.regularizedGammaQ(double,double) H4] tokens: 353 prompt + 3 completion = 356 total
  📊 GPT[method_explanation org.apache.commons.math.special.Gamma.regularizedGammaQ(double,double) H4] tokens: 328 prompt + 124 completion = 452 total
  📊 GPT[method_score org.apache.commons.math.special.Gamma.regularizedGammaQ(double,double,double,int) H4] tokens: 379 prompt + 3 completion = 382 total
  📊 GPT[method_explanation org.apache.commons.math.special.Gamma.regularizedGammaQ(double,double,double,int) H4] tokens: 354 prompt + 156 completion = 510 total
  📊 GPT[method_score org.apache.commons.math.special.Gamma.logGamma(double) H4] tokens: 338 prompt + 3 completion = 341 total
  📊 GPT[method_explanation org.apache.commons.math.special.Gamma.logGamma(double) H4] tokens: 313 prompt + 115 completion = 428 total
  📊 GPT[method_score org.apache.commons.math.special.Gamma.regularizedGammaP(double,double) H4] tokens: 404 prompt + 3 completion = 407 total
  📊 GPT[method_explanation org.apache.commons.math.special.Gamma.regularizedGammaP(double,double) H4] tokens: 382 prompt + 119 completion = 501 total
  📊 GPT[method_score org.apache.commons.math.util.ContinuedFraction.evaluate(double,double,int) H5] tokens: 532 prompt + 3 completion = 535 total
  📊 GPT[method_explanation org.apache.commons.math.util.ContinuedFraction.evaluate(double,double,int) H5] tokens: 394 prompt + 130 completion = 524 total
  📊 GPT[method_score org.apache.commons.math.special.Gamma.regularizedGammaP(double,double,double,int) H5] tokens: 488 prompt + 3 completion = 491 total
  📊 GPT[method_explanation org.apache.commons.math.special.Gamma.regularizedGammaP(double,double,double,int) H5] tokens: 349 prompt + 146 completion = 495 total
  📊 GPT[method_score org.apache.commons.math.special.Gamma.regularizedGammaQ(double,double) H5] tokens: 353 prompt + 3 completion = 356 total
  📊 GPT[method_explanation org.apache.commons.math.special.Gamma.regularizedGammaQ(double,double) H5] tokens: 328 prompt + 150 completion = 478 total
  📊 GPT[method_score org.apache.commons.math.special.Gamma.regularizedGammaQ(double,double,double,int) H5] tokens: 379 prompt + 3 completion = 382 total
  📊 GPT[method_explanation org.apache.commons.math.special.Gamma.regularizedGammaQ(double,double,double,int) H5] tokens: 354 prompt + 151 completion = 505 total
  📊 GPT[method_score org.apache.commons.math.special.Gamma.logGamma(double) H5] tokens: 338 prompt + 3 completion = 341 total
  📊 GPT[method_explanation org.apache.commons.math.special.Gamma.logGamma(double) H5] tokens: 313 prompt + 125 completion = 438 total
  📊 GPT[method_score org.apache.commons.math.special.Gamma.regularizedGammaP(double,double) H5] tokens: 404 prompt + 3 completion = 407 total
  📊 GPT[method_explanation org.apache.commons.math.special.Gamma.regularizedGammaP(double,double) H5] tokens: 382 prompt + 104 completion = 486 total

Top suspicious methods:
  1. org.apache.commons.math.special.Gamma.regularizedGammaP(double,double): 0.850 — best hypothesis H1: Hypothesis H1: The failure in "org.apache.commons.math.special.GammaTest::testRegularizedGammaPositivePositive" could be due to incorrect handling of edge cases where the input values are very close to zero, leading to numerical instability or precision errors in the regularized gamma function calculation. (confidence 0.700); supporting class org.apache.commons.math.special.Gamma (HH1)
      explanation: The method `regularizedGammaP(double a, double x)` calls `regularizedGammaP(a, x, DEFAULT_EPSILON, Integer.MAX_VALUE)`, which suggests it uses a default precision (`DEFAULT_EPSILON`) and a high iteration limit (`Integer.MAX_VALUE`) to co...
  2. org.apache.commons.math.special.Gamma.regularizedGammaP(double,double,double,int): 0.800 — best hypothesis H1: Hypothesis H1: The failure in "org.apache.commons.math.special.GammaTest::testRegularizedGammaPositivePositive" could be due to incorrect handling of edge cases where the input values are very close to zero, leading to numerical instability or precision errors in the regularized gamma function calculation. (confidence 0.700); supporting class org.apache.commons.math.special.Gamma (HH1)
      explanation: The method `org.apache.commons.math.special.Gamma.regularizedGammaP(double a, double x, double epsilon, int maxIterations)` calculates the regularized gamma function P(a, x) and is sensitive to precision errors, especially when `a` and `...
  3. org.apache.commons.math.special.Gamma.regularizedGammaQ(double,double): 0.800 — best hypothesis H1: Hypothesis H1: The failure in "org.apache.commons.math.special.GammaTest::testRegularizedGammaPositivePositive" could be due to incorrect handling of edge cases where the input values are very close to zero, leading to numerical instability or precision errors in the regularized gamma function calculation. (confidence 0.700); supporting class org.apache.commons.math.special.Gamma (HH1)
      explanation: The method `org.apache.commons.math.special.Gamma.regularizedGammaQ(double, double)` supports Hypothesis H1 as it delegates the computation to an overloaded method with default precision (`epsilon`) and iteration limits (`maxIterations`)...
  4. org.apache.commons.math.special.Gamma.regularizedGammaQ(double,double,double,int): 0.800 — best hypothesis H1: Hypothesis H1: The failure in "org.apache.commons.math.special.GammaTest::testRegularizedGammaPositivePositive" could be due to incorrect handling of edge cases where the input values are very close to zero, leading to numerical instability or precision errors in the regularized gamma function calculation. (confidence 0.700); supporting class org.apache.commons.math.special.Gamma (HH1)
      explanation: The method `org.apache.commons.math.special.Gamma.regularizedGammaQ(double, double, double, int)` supports hypothesis H1 as it involves computations that can be sensitive to numerical precision, especially when handling values close to z...
  5. org.apache.commons.math.util.ContinuedFraction.evaluate(double,double,int): 0.700 — best hypothesis H1: Hypothesis H1: The failure in "org.apache.commons.math.special.GammaTest::testRegularizedGammaPositivePositive" could be due to incorrect handling of edge cases where the input values are very close to zero, leading to numerical instability or precision errors in the regularized gamma function calculation. (confidence 0.700); supporting class org.apache.commons.math.util.ContinuedFraction (HH1)
      explanation: The method `org.apache.commons.math.util.ContinuedFraction.evaluate(double, double, int)` is responsible for evaluating a continued fraction at a given value `x`, which is a critical part of calculating functions like the regularized gam...
  6. org.apache.commons.math.special.Gamma.logGamma(double): 0.300 — best hypothesis H1: Hypothesis H1: The failure in "org.apache.commons.math.special.GammaTest::testRegularizedGammaPositivePositive" could be due to incorrect handling of edge cases where the input values are very close to zero, leading to numerical instability or precision errors in the regularized gamma function calculation. (confidence 0.700); supporting class org.apache.commons.math.special.Gamma (HH1)
      explanation: The method `org.apache.commons.math.special.Gamma.logGamma(double)` computes the natural logarithm of the gamma function using the Lanczos approximation, which is generally stable for a wide range of input values, including those close t...

📊 Token Usage Summary:
  Total API calls: 98
  Total tokens: 41,224
  Prompt tokens: 35,128
  Completion tokens: 6,096
Results written to defects4j_batch_results/Math-104_parallel_case/Math-104_parallel_answer.csv
Token usage written to defects4j_batch_results/Math-104_parallel_case/Math-104_token_usage.csv
Summary written to defects4j_batch_results/Math-104_parallel_case/Math-104_parallel_summary.md
