=== GPT-only pipeline for Math-93 ===
  📊 GPT[hypothesis H1] tokens: 74 prompt + 43 completion = 117 total
  📊 GPT[hypothesis H2] tokens: 74 prompt + 39 completion = 113 total
  📊 GPT[hypothesis H3] tokens: 74 prompt + 44 completion = 118 total
  📊 GPT[hypothesis H4] tokens: 74 prompt + 44 completion = 118 total
  📊 GPT[hypothesis H5] tokens: 74 prompt + 32 completion = 106 total
  📊 GPT[hypothesis_confidence H1] tokens: 104 prompt + 3 completion = 107 total
  📊 GPT[hypothesis_confidence H2] tokens: 100 prompt + 3 completion = 103 total
  📊 GPT[hypothesis_confidence H3] tokens: 105 prompt + 3 completion = 108 total
  📊 GPT[hypothesis_confidence H4] tokens: 105 prompt + 3 completion = 108 total
  📊 GPT[hypothesis_confidence H5] tokens: 93 prompt + 3 completion = 96 total
Hypotheses:
  H1 (confidence 0.800): H1: The failure in "org.apache.commons.math.util.MathUtilsTest::testFactorial" could be caused by an integer overflow when calculating the factorial of a large number, resulting in incorrect or negative values.
  H2 (confidence 0.800): Hypothesis H2: The failure in "org.apache.commons.math.util.MathUtilsTest::testFactorial" could be due to an integer overflow occurring when calculating the factorial of a large number.
  H3 (confidence 0.800): Hypothesis H3: The failure in "org.apache.commons.math.util.MathUtilsTest::testFactorial" could be due to an integer overflow occurring when calculating the factorial of a large number, resulting in incorrect values.
  H4 (confidence 0.800): Hypothesis H4: The failure in "org.apache.commons.math.util.MathUtilsTest::testFactorial" might be due to an integer overflow occurring when calculating the factorial of a large number, resulting in incorrect values.
  H5 (confidence 0.700): Hypothesis H5: The failure might be caused by an integer overflow occurring when calculating the factorial of a large number, exceeding the data type's maximum limit.
    ▶️ GPT[class pre-ranking] running 1 prompts
  📊 GPT[class_pre_rank org.apache.commons.math.util.MathUtils] tokens: 642 prompt + 59 completion = 701 total
    ✅ GPT[class pre-ranking] completed; taking top 5
Candidate classes:
  org.apache.commons.math.util.MathUtils: n/a ```json
{"score": 0.9, "reason": "The failure occurs in MathUtils.factorialDouble(int), which is part of the MathUtils class. The discrepancy in expected and actual values suggests a precision issue, making this class the likely location for the fix."}
```
Collected 3 methods across candidate classes
    ▶️ GPT[method pre-ranking] running 3 prompts
  📊 GPT[method_pre_rank org.apache.commons.math.util.MathUtils.factorial(int)] tokens: 672 prompt + 80 completion = 752 total
  📊 GPT[method_pre_rank org.apache.commons.math.util.MathUtils.factorialDouble(int)] tokens: 686 prompt + 83 completion = 769 total
  📊 GPT[method_pre_rank org.apache.commons.math.util.MathUtils.factorialLog(int)] tokens: 774 prompt + 71 completion = 845 total
    ✅ GPT[method pre-ranking] completed
Selected 3 candidate methods
  📊 GPT[class_score org.apache.commons.math.util.MathUtils H1] tokens: 450 prompt + 3 completion = 453 total
  📊 GPT[class_explanation org.apache.commons.math.util.MathUtils H1] tokens: 428 prompt + 120 completion = 548 total
  📊 GPT[class_score org.apache.commons.math.util.MathUtils H2] tokens: 446 prompt + 3 completion = 449 total
  📊 GPT[class_explanation org.apache.commons.math.util.MathUtils H2] tokens: 424 prompt + 118 completion = 542 total
  📊 GPT[class_score org.apache.commons.math.util.MathUtils H3] tokens: 451 prompt + 3 completion = 454 total
  📊 GPT[class_explanation org.apache.commons.math.util.MathUtils H3] tokens: 429 prompt + 116 completion = 545 total
  📊 GPT[class_score org.apache.commons.math.util.MathUtils H4] tokens: 451 prompt + 3 completion = 454 total
  📊 GPT[class_explanation org.apache.commons.math.util.MathUtils H4] tokens: 429 prompt + 100 completion = 529 total
  📊 GPT[class_score org.apache.commons.math.util.MathUtils H5] tokens: 439 prompt + 3 completion = 442 total
  📊 GPT[class_explanation org.apache.commons.math.util.MathUtils H5] tokens: 417 prompt + 136 completion = 553 total
  📊 GPT[method_score org.apache.commons.math.util.MathUtils.factorialDouble(int) H1] tokens: 521 prompt + 3 completion = 524 total
  📊 GPT[method_explanation org.apache.commons.math.util.MathUtils.factorialDouble(int) H1] tokens: 487 prompt + 115 completion = 602 total
  📊 GPT[method_score org.apache.commons.math.util.MathUtils.factorial(int) H1] tokens: 503 prompt + 3 completion = 506 total
  📊 GPT[method_explanation org.apache.commons.math.util.MathUtils.factorial(int) H1] tokens: 477 prompt + 103 completion = 580 total
  📊 GPT[method_score org.apache.commons.math.util.MathUtils.factorialLog(int) H1] tokens: 609 prompt + 3 completion = 612 total
  📊 GPT[method_explanation org.apache.commons.math.util.MathUtils.factorialLog(int) H1] tokens: 556 prompt + 125 completion = 681 total
  📊 GPT[method_score org.apache.commons.math.util.MathUtils.factorialDouble(int) H2] tokens: 517 prompt + 3 completion = 520 total
  📊 GPT[method_explanation org.apache.commons.math.util.MathUtils.factorialDouble(int) H2] tokens: 483 prompt + 135 completion = 618 total
  📊 GPT[method_score org.apache.commons.math.util.MathUtils.factorial(int) H2] tokens: 499 prompt + 3 completion = 502 total
  📊 GPT[method_explanation org.apache.commons.math.util.MathUtils.factorial(int) H2] tokens: 473 prompt + 116 completion = 589 total
  📊 GPT[method_score org.apache.commons.math.util.MathUtils.factorialLog(int) H2] tokens: 605 prompt + 3 completion = 608 total
  📊 GPT[method_explanation org.apache.commons.math.util.MathUtils.factorialLog(int) H2] tokens: 552 prompt + 127 completion = 679 total
  📊 GPT[method_score org.apache.commons.math.util.MathUtils.factorialDouble(int) H3] tokens: 522 prompt + 3 completion = 525 total
  📊 GPT[method_explanation org.apache.commons.math.util.MathUtils.factorialDouble(int) H3] tokens: 488 prompt + 146 completion = 634 total
  📊 GPT[method_score org.apache.commons.math.util.MathUtils.factorial(int) H3] tokens: 504 prompt + 3 completion = 507 total
  📊 GPT[method_explanation org.apache.commons.math.util.MathUtils.factorial(int) H3] tokens: 478 prompt + 121 completion = 599 total
  📊 GPT[method_score org.apache.commons.math.util.MathUtils.factorialLog(int) H3] tokens: 610 prompt + 3 completion = 613 total
  📊 GPT[method_explanation org.apache.commons.math.util.MathUtils.factorialLog(int) H3] tokens: 557 prompt + 126 completion = 683 total
  📊 GPT[method_score org.apache.commons.math.util.MathUtils.factorialDouble(int) H4] tokens: 522 prompt + 3 completion = 525 total
  📊 GPT[method_explanation org.apache.commons.math.util.MathUtils.factorialDouble(int) H4] tokens: 488 prompt + 124 completion = 612 total
  📊 GPT[method_score org.apache.commons.math.util.MathUtils.factorial(int) H4] tokens: 504 prompt + 3 completion = 507 total
  📊 GPT[method_explanation org.apache.commons.math.util.MathUtils.factorial(int) H4] tokens: 478 prompt + 110 completion = 588 total
  📊 GPT[method_score org.apache.commons.math.util.MathUtils.factorialLog(int) H4] tokens: 610 prompt + 3 completion = 613 total
  📊 GPT[method_explanation org.apache.commons.math.util.MathUtils.factorialLog(int) H4] tokens: 557 prompt + 134 completion = 691 total
  📊 GPT[method_score org.apache.commons.math.util.MathUtils.factorialDouble(int) H5] tokens: 510 prompt + 3 completion = 513 total
  📊 GPT[method_explanation org.apache.commons.math.util.MathUtils.factorialDouble(int) H5] tokens: 476 prompt + 99 completion = 575 total
  📊 GPT[method_score org.apache.commons.math.util.MathUtils.factorial(int) H5] tokens: 492 prompt + 3 completion = 495 total
  📊 GPT[method_explanation org.apache.commons.math.util.MathUtils.factorial(int) H5] tokens: 466 prompt + 111 completion = 577 total
  📊 GPT[method_score org.apache.commons.math.util.MathUtils.factorialLog(int) H5] tokens: 598 prompt + 3 completion = 601 total
  📊 GPT[method_explanation org.apache.commons.math.util.MathUtils.factorialLog(int) H5] tokens: 545 prompt + 112 completion = 657 total

Top suspicious methods:
  1. org.apache.commons.math.util.MathUtils.factorialDouble(int): 0.800 — best hypothesis H1: H1: The failure in "org.apache.commons.math.util.MathUtilsTest::testFactorial" could be caused by an integer overflow when calculating the factorial of a large number, resulting in incorrect or negative values. (confidence 0.800); supporting class org.apache.commons.math.util.MathUtils (HH1)
      explanation: The method `MathUtils.factorialDouble(int)` returns the factorial of a number `n` as a `double`, which inherently supports larger values than an `int` and reduces the risk of integer overflow. The failure in the test occurs due to a prec...
  2. org.apache.commons.math.util.MathUtils.factorial(int): 0.300 — best hypothesis H1: H1: The failure in "org.apache.commons.math.util.MathUtilsTest::testFactorial" could be caused by an integer overflow when calculating the factorial of a large number, resulting in incorrect or negative values. (confidence 0.800); supporting class org.apache.commons.math.util.MathUtils (HH1)
      explanation: The method `org.apache.commons.math.util.MathUtils.factorial(int)` does not support hypothesis H1 because it computes the factorial using a precomputed "factorials" array and a double approximation, which avoids integer overflow issues. ...
  3. org.apache.commons.math.util.MathUtils.factorialLog(int): 0.100 — best hypothesis H1: H1: The failure in "org.apache.commons.math.util.MathUtilsTest::testFactorial" could be caused by an integer overflow when calculating the factorial of a large number, resulting in incorrect or negative values. (confidence 0.800); supporting class org.apache.commons.math.util.MathUtils (HH1)
      explanation: The method `MathUtils.factorialLog(int)` calculates the natural logarithm of a factorial, which inherently avoids integer overflow by working with logarithmic values rather than directly computing large factorials. This approach supports...

📊 Token Usage Summary:
  Total API calls: 54
  Total tokens: 26,666
  Prompt tokens: 23,702
  Completion tokens: 2,964
Results written to defects4j_batch_results/Math-93_parallel_case/Math-93_parallel_answer.csv
Token usage written to defects4j_batch_results/Math-93_parallel_case/Math-93_token_usage.csv
Summary written to defects4j_batch_results/Math-93_parallel_case/Math-93_parallel_summary.md
