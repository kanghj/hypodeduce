=== GPT-only pipeline for Math-89 ===
  📊 GPT[hypothesis H1] tokens: 75 prompt + 49 completion = 124 total
  📊 GPT[hypothesis H2] tokens: 75 prompt + 43 completion = 118 total
  📊 GPT[hypothesis H3] tokens: 75 prompt + 37 completion = 112 total
  📊 GPT[hypothesis H4] tokens: 75 prompt + 37 completion = 112 total
  📊 GPT[hypothesis H5] tokens: 75 prompt + 40 completion = 115 total
  📊 GPT[hypothesis_confidence H1] tokens: 110 prompt + 3 completion = 113 total
  📊 GPT[hypothesis_confidence H2] tokens: 104 prompt + 3 completion = 107 total
  📊 GPT[hypothesis_confidence H3] tokens: 98 prompt + 3 completion = 101 total
  📊 GPT[hypothesis_confidence H4] tokens: 98 prompt + 3 completion = 101 total
  📊 GPT[hypothesis_confidence H5] tokens: 101 prompt + 3 completion = 104 total
Hypotheses:
  H1 (confidence 0.800): Hypothesis H1: The failure in "org.apache.commons.math.stat.FrequencyTest::testAddNonComparable" may be caused by the method incorrectly attempting to compare non-comparable objects, leading to a ClassCastException or similar runtime error.
  H2 (confidence 0.700): Hypothesis H2: The failure may be caused by the `Frequency` class not properly handling or rejecting non-comparable objects, leading to a `ClassCastException` or similar runtime error during the test execution.
  H3 (confidence 0.700): Hypothesis H3: The failure may be caused by the `Frequency` class not properly handling or rejecting non-comparable objects, leading to a runtime exception when attempting to add them.
  H4 (confidence 0.700): Hypothesis H4: The failure may be caused by the `Frequency` class not properly handling or rejecting non-comparable objects, leading to a runtime exception when attempting to add them.
  H5 (confidence 0.700): Hypothesis H5: The failure may be caused by an attempt to add elements to the Frequency object that do not implement the Comparable interface, leading to a ClassCastException during sorting or comparison operations.
    ▶️ GPT[class pre-ranking] running 1 prompts
  📊 GPT[class_pre_rank org.apache.commons.math.stat.Frequency] tokens: 527 prompt + 67 completion = 594 total
    ✅ GPT[class pre-ranking] completed; taking top 5
Candidate classes:
  org.apache.commons.math.stat.Frequency: n/a ```json
{"score": 0.9, "reason": "The failure occurs in the addValue(Object) method of the Frequency class, which throws a ClassCastException instead of the expected IllegalArgumentException when adding non-Comparable objects. This indicates the bug is likely in this method's handling of object types."}
```
Collected 2 methods across candidate classes
    ▶️ GPT[method pre-ranking] running 2 prompts
  📊 GPT[method_pre_rank org.apache.commons.math.stat.Frequency.Frequency()] tokens: 525 prompt + 68 completion = 593 total
  📊 GPT[method_pre_rank org.apache.commons.math.stat.Frequency.addValue(Object)] tokens: 615 prompt + 60 completion = 675 total
    ✅ GPT[method pre-ranking] completed
Selected 2 candidate methods
  📊 GPT[class_score org.apache.commons.math.stat.Frequency H1] tokens: 333 prompt + 3 completion = 336 total
  📊 GPT[class_explanation org.apache.commons.math.stat.Frequency H1] tokens: 310 prompt + 124 completion = 434 total
  📊 GPT[class_score org.apache.commons.math.stat.Frequency H2] tokens: 327 prompt + 3 completion = 330 total
  📊 GPT[class_explanation org.apache.commons.math.stat.Frequency H2] tokens: 304 prompt + 96 completion = 400 total
  📊 GPT[class_score org.apache.commons.math.stat.Frequency H3] tokens: 321 prompt + 3 completion = 324 total
  📊 GPT[class_explanation org.apache.commons.math.stat.Frequency H3] tokens: 298 prompt + 108 completion = 406 total
  📊 GPT[class_score org.apache.commons.math.stat.Frequency H4] tokens: 321 prompt + 3 completion = 324 total
  📊 GPT[class_explanation org.apache.commons.math.stat.Frequency H4] tokens: 298 prompt + 103 completion = 401 total
  📊 GPT[class_score org.apache.commons.math.stat.Frequency H5] tokens: 324 prompt + 3 completion = 327 total
  📊 GPT[class_explanation org.apache.commons.math.stat.Frequency H5] tokens: 301 prompt + 116 completion = 417 total
  📊 GPT[method_score org.apache.commons.math.stat.Frequency.addValue(Object) H1] tokens: 443 prompt + 3 completion = 446 total
  📊 GPT[method_explanation org.apache.commons.math.stat.Frequency.addValue(Object) H1] tokens: 407 prompt + 109 completion = 516 total
  📊 GPT[method_score org.apache.commons.math.stat.Frequency.Frequency() H1] tokens: 350 prompt + 3 completion = 353 total
  📊 GPT[method_explanation org.apache.commons.math.stat.Frequency.Frequency() H1] tokens: 325 prompt + 107 completion = 432 total
  📊 GPT[method_score org.apache.commons.math.stat.Frequency.addValue(Object) H2] tokens: 437 prompt + 3 completion = 440 total
  📊 GPT[method_explanation org.apache.commons.math.stat.Frequency.addValue(Object) H2] tokens: 401 prompt + 111 completion = 512 total
  📊 GPT[method_score org.apache.commons.math.stat.Frequency.Frequency() H2] tokens: 344 prompt + 3 completion = 347 total
  📊 GPT[method_explanation org.apache.commons.math.stat.Frequency.Frequency() H2] tokens: 319 prompt + 104 completion = 423 total
  📊 GPT[method_score org.apache.commons.math.stat.Frequency.addValue(Object) H3] tokens: 431 prompt + 3 completion = 434 total
  📊 GPT[method_explanation org.apache.commons.math.stat.Frequency.addValue(Object) H3] tokens: 395 prompt + 117 completion = 512 total
  📊 GPT[method_score org.apache.commons.math.stat.Frequency.Frequency() H3] tokens: 338 prompt + 3 completion = 341 total
  📊 GPT[method_explanation org.apache.commons.math.stat.Frequency.Frequency() H3] tokens: 313 prompt + 101 completion = 414 total
  📊 GPT[method_score org.apache.commons.math.stat.Frequency.addValue(Object) H4] tokens: 431 prompt + 3 completion = 434 total
  📊 GPT[method_explanation org.apache.commons.math.stat.Frequency.addValue(Object) H4] tokens: 395 prompt + 108 completion = 503 total
  📊 GPT[method_score org.apache.commons.math.stat.Frequency.Frequency() H4] tokens: 338 prompt + 3 completion = 341 total
  📊 GPT[method_explanation org.apache.commons.math.stat.Frequency.Frequency() H4] tokens: 313 prompt + 101 completion = 414 total
  📊 GPT[method_score org.apache.commons.math.stat.Frequency.addValue(Object) H5] tokens: 434 prompt + 3 completion = 437 total
  📊 GPT[method_explanation org.apache.commons.math.stat.Frequency.addValue(Object) H5] tokens: 398 prompt + 132 completion = 530 total
  📊 GPT[method_score org.apache.commons.math.stat.Frequency.Frequency() H5] tokens: 341 prompt + 3 completion = 344 total
  📊 GPT[method_explanation org.apache.commons.math.stat.Frequency.Frequency() H5] tokens: 316 prompt + 118 completion = 434 total

Top suspicious methods:
  1. org.apache.commons.math.stat.Frequency.addValue(Object): 0.900 — best hypothesis H1: Hypothesis H1: The failure in "org.apache.commons.math.stat.FrequencyTest::testAddNonComparable" may be caused by the method incorrectly attempting to compare non-comparable objects, leading to a ClassCastException or similar runtime error. (confidence 0.800); supporting class org.apache.commons.math.stat.Frequency (HH1)
      explanation: The method `org.apache.commons.math.stat.Frequency.addValue(Object)` supports Hypothesis H1. The method attempts to cast the input object `v` to `Comparable<?>` using `(Comparable<?>) v`, which will throw a `ClassCastException` if `v` is...
  2. org.apache.commons.math.stat.Frequency.Frequency(): 0.200 — best hypothesis H1: Hypothesis H1: The failure in "org.apache.commons.math.stat.FrequencyTest::testAddNonComparable" may be caused by the method incorrectly attempting to compare non-comparable objects, leading to a ClassCastException or similar runtime error. (confidence 0.800); supporting class org.apache.commons.math.stat.Frequency (HH1)
      explanation: The method `org.apache.commons.math.stat.Frequency.Frequency()` initializes the frequency table using a `TreeMap`, which inherently requires keys to be `Comparable`. This supports hypothesis H1, as the failure occurs when non-comparable ...

📊 Token Usage Summary:
  Total API calls: 43
  Total tokens: 15,275
  Prompt tokens: 13,159
  Completion tokens: 2,116
Results written to defects4j_batch_results/Math-89_parallel_case/Math-89_parallel_answer.csv
Token usage written to defects4j_batch_results/Math-89_parallel_case/Math-89_token_usage.csv
Summary written to defects4j_batch_results/Math-89_parallel_case/Math-89_parallel_summary.md
